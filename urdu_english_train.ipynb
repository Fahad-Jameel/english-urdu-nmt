{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSPNFbLiyXoN",
        "outputId": "8da1fa9e-afab-4ebd-f428-2793988917ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "English corpus length: 24525\n",
            "Urdu corpus length: 24525\n",
            "\n",
            "Dataset size: 24525 sentence pairs\n",
            "\n",
            "Sample data (first 5 rows):\n",
            "                english                       urdu\n",
            "0   is zain your nephew      زین تمہارا بھتیجا ہے۔\n",
            "1  i wish youd trust me  کاش تم مجھ پر بھروسہ کرتے\n",
            "2      did he touch you      کیا اس نے آپ کو چھوا؟\n",
            "3      its part of life         اس کی زندگی کا حصہ\n",
            "4        zain isnt ugly        زین بدصورت نہیں ہے۔\n",
            "\n",
            "--- Data Visualization ---\n",
            "English sentences - Average length: 4.02 words, Max length: 16 words\n",
            "Urdu sentences - Average length: 5.03 words, Max length: 19 words\n",
            "Correlation between English and Urdu sentence lengths: 0.6516\n",
            "\n",
            "English vocabulary size: 5766 unique words\n",
            "Urdu vocabulary size: 5986 unique words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-984803ff28a3>:142: UserWarning: Glyph 1729 (\\N{ARABIC LETTER HEH GOAL}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-1-984803ff28a3>:142: UserWarning: Matplotlib currently does not support Arabic natively.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-1-984803ff28a3>:142: UserWarning: Glyph 1746 (\\N{ARABIC LETTER YEH BARREE}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-1-984803ff28a3>:142: UserWarning: Glyph 1748 (\\N{ARABIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-1-984803ff28a3>:143: UserWarning: Glyph 1729 (\\N{ARABIC LETTER HEH GOAL}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig('visualizations/top_words_frequency.png')\n",
            "<ipython-input-1-984803ff28a3>:143: UserWarning: Matplotlib currently does not support Arabic natively.\n",
            "  plt.savefig('visualizations/top_words_frequency.png')\n",
            "<ipython-input-1-984803ff28a3>:143: UserWarning: Glyph 1746 (\\N{ARABIC LETTER YEH BARREE}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig('visualizations/top_words_frequency.png')\n",
            "<ipython-input-1-984803ff28a3>:143: UserWarning: Glyph 1748 (\\N{ARABIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig('visualizations/top_words_frequency.png')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "import time\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Load the dataset from text files\n",
        "with open('english-corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    english_sentences = f.read().splitlines()\n",
        "\n",
        "with open('urdu-corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    urdu_sentences = f.read().splitlines()\n",
        "\n",
        "print(f\"English corpus length: {len(english_sentences)}\")\n",
        "print(f\"Urdu corpus length: {len(urdu_sentences)}\")\n",
        "\n",
        "if len(english_sentences) != len(urdu_sentences):\n",
        "    print(\"Warning: The number of sentences in English and Urdu files don't match!\")\n",
        "    min_len = min(len(english_sentences), len(urdu_sentences))\n",
        "    english_sentences = english_sentences[:min_len]\n",
        "    urdu_sentences = urdu_sentences[:min_len]\n",
        "\n",
        "# Create a dataframe for easier handling\n",
        "df = pd.DataFrame({\n",
        "    'english': english_sentences,\n",
        "    'urdu': urdu_sentences\n",
        "})\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"\\nDataset size: {len(df)} sentence pairs\")\n",
        "print(\"\\nSample data (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# Data visualization\n",
        "print(\"\\n--- Data Visualization ---\")\n",
        "\n",
        "# Sentence length distribution\n",
        "eng_lens = [len(text.split()) for text in df['english']]\n",
        "urdu_lens = [len(text.split()) for text in df['urdu']]\n",
        "\n",
        "# Create directory for visualizations\n",
        "if not os.path.exists('visualizations'):\n",
        "    os.makedirs('visualizations')\n",
        "\n",
        "# Plot histogram of sentence lengths\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(eng_lens, bins=30, color='blue', alpha=0.7)\n",
        "plt.title('English Sentence Length Distribution')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(urdu_lens, bins=30, color='green', alpha=0.7)\n",
        "plt.title('Urdu Sentence Length Distribution')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/sentence_length_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Print statistics\n",
        "print(f\"English sentences - Average length: {np.mean(eng_lens):.2f} words, Max length: {max(eng_lens)} words\")\n",
        "print(f\"Urdu sentences - Average length: {np.mean(urdu_lens):.2f} words, Max length: {max(urdu_lens)} words\")\n",
        "\n",
        "# Calculate and visualize the relationship between English and Urdu sentence lengths\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(eng_lens, urdu_lens, alpha=0.3)\n",
        "plt.title('English vs Urdu Sentence Lengths')\n",
        "plt.xlabel('English Sentence Length (words)')\n",
        "plt.ylabel('Urdu Sentence Length (words)')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('visualizations/eng_vs_urdu_lengths.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate correlation between English and Urdu sentence lengths\n",
        "correlation = np.corrcoef(eng_lens, urdu_lens)[0, 1]\n",
        "print(f\"Correlation between English and Urdu sentence lengths: {correlation:.4f}\")\n",
        "\n",
        "# Analyze vocabulary\n",
        "eng_vocab = set()\n",
        "urdu_vocab = set()\n",
        "\n",
        "for sent in english_sentences:\n",
        "    for word in sent.split():\n",
        "        eng_vocab.add(word.lower())\n",
        "\n",
        "for sent in urdu_sentences:\n",
        "    for word in sent.split():\n",
        "        urdu_vocab.add(word)\n",
        "\n",
        "print(f\"\\nEnglish vocabulary size: {len(eng_vocab)} unique words\")\n",
        "print(f\"Urdu vocabulary size: {len(urdu_vocab)} unique words\")\n",
        "\n",
        "# Plot top 20 most common words in each language\n",
        "from collections import Counter\n",
        "\n",
        "eng_word_freq = Counter()\n",
        "urdu_word_freq = Counter()\n",
        "\n",
        "for sent in english_sentences:\n",
        "    eng_word_freq.update(sent.lower().split())\n",
        "\n",
        "for sent in urdu_sentences:\n",
        "    urdu_word_freq.update(sent.split())\n",
        "\n",
        "top_eng_words = dict(eng_word_freq.most_common(20))\n",
        "top_urdu_words = dict(urdu_word_freq.most_common(20))\n",
        "\n",
        "# Plot for English\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(top_eng_words)), list(top_eng_words.values()), align='center')\n",
        "plt.xticks(range(len(top_eng_words)), list(top_eng_words.keys()), rotation=90)\n",
        "plt.title('Top 20 English Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot for Urdu\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(top_urdu_words)), list(top_urdu_words.values()), align='center')\n",
        "plt.xticks(range(len(top_urdu_words)), list(top_urdu_words.keys()), rotation=90)\n",
        "plt.title('Top 20 Urdu Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/top_words_frequency.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Data Preprocessing ---\")\n",
        "\n",
        "# Define preprocessing functions\n",
        "def preprocess_english(text):\n",
        "    \"\"\"Preprocess English text for translation\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Add start and end tokens\n",
        "    return '<start> ' + text + ' <end>'\n",
        "\n",
        "def preprocess_urdu(text):\n",
        "    \"\"\"Preprocess Urdu text for translation\"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Add start and end tokens\n",
        "    return '<start> ' + text + ' <end>'\n",
        "\n",
        "# Apply preprocessing\n",
        "df['english_processed'] = df['english'].apply(preprocess_english)\n",
        "df['urdu_processed'] = df['urdu'].apply(preprocess_urdu)\n",
        "\n",
        "print(\"Example of preprocessed data:\")\n",
        "print(\"English original:\", df['english'].iloc[0])\n",
        "print(\"English processed:\", df['english_processed'].iloc[0])\n",
        "print(\"Urdu original:\", df['urdu'].iloc[0])\n",
        "print(\"Urdu processed:\", df['urdu_processed'].iloc[0])\n",
        "\n",
        "\n",
        "print(\"\\n--- Tokenization ---\")\n",
        "\n",
        "\n",
        "eng_tokenizer = Tokenizer(filters='')\n",
        "urdu_tokenizer = Tokenizer(filters='')\n",
        "\n",
        "# Fit tokenizers on processed text\n",
        "eng_tokenizer.fit_on_texts(df['english_processed'])\n",
        "urdu_tokenizer.fit_on_texts(df['urdu_processed'])\n",
        "\n",
        "# Calculate vocabulary sizes\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "urdu_vocab_size = len(urdu_tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"English tokenized vocabulary size: {eng_vocab_size}\")\n",
        "print(f\"Urdu tokenized vocabulary size: {urdu_vocab_size}\")\n",
        "\n",
        "# Save tokenizers for later use\n",
        "os.makedirs('models', exist_ok=True)\n",
        "with open('models/eng_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('models/urdu_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(urdu_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Convert sentences to sequences\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(df['english_processed'])\n",
        "urdu_sequences = urdu_tokenizer.texts_to_sequences(df['urdu_processed'])\n",
        "\n",
        "# Analyze sequence lengths\n",
        "eng_seq_lens = [len(seq) for seq in eng_sequences]\n",
        "urdu_seq_lens = [len(seq) for seq in urdu_sequences]\n",
        "\n",
        "# Determine maximum sequence lengths (cap at 50 for efficiency)\n",
        "max_eng_length = min(max(eng_seq_lens), 50)\n",
        "max_urdu_length = min(max(urdu_seq_lens), 50)\n",
        "\n",
        "print(f\"Maximum English sequence length (capped): {max_eng_length}\")\n",
        "print(f\"Maximum Urdu sequence length (capped): {max_urdu_length}\")\n",
        "\n",
        "# Visualize sequences that will be truncated\n",
        "eng_truncated = sum(1 for x in eng_seq_lens if x > max_eng_length)\n",
        "urdu_truncated = sum(1 for x in urdu_seq_lens if x > max_urdu_length)\n",
        "print(f\"English sequences that will be truncated: {eng_truncated} ({eng_truncated/len(eng_seq_lens)*100:.2f}%)\")\n",
        "print(f\"Urdu sequences that will be truncated: {urdu_truncated} ({urdu_truncated/len(urdu_seq_lens)*100:.2f}%)\")\n",
        "\n",
        "# Pad sequences\n",
        "encoder_input_data = pad_sequences(eng_sequences, maxlen=max_eng_length, padding='post')\n",
        "decoder_input_data = pad_sequences(urdu_sequences, maxlen=max_urdu_length, padding='post')\n",
        "\n",
        "# Create decoder target data (shift by one position)\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]  # Shift left by one position\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: 80% for training+validation, 20% for testing\n",
        "train_val_idx, test_idx = train_test_split(\n",
        "    range(len(df)), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    train_val_idx, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Extract data for each set\n",
        "x_train = encoder_input_data[train_idx]\n",
        "x_val = encoder_input_data[val_idx]\n",
        "x_test = encoder_input_data[test_idx]\n",
        "\n",
        "y_train_in = decoder_input_data[train_idx]\n",
        "y_val_in = decoder_input_data[val_idx]\n",
        "y_test_in = decoder_input_data[test_idx]\n",
        "\n",
        "y_train_target = decoder_target_data[train_idx]\n",
        "y_val_target = decoder_target_data[val_idx]\n",
        "y_test_target = decoder_target_data[test_idx]\n",
        "\n",
        "# Save original sentences for later evaluation\n",
        "train_eng_texts = df['english'].iloc[train_idx].reset_index(drop=True)\n",
        "train_urdu_texts = df['urdu'].iloc[train_idx].reset_index(drop=True)\n",
        "val_eng_texts = df['english'].iloc[val_idx].reset_index(drop=True)\n",
        "val_urdu_texts = df['urdu'].iloc[val_idx].reset_index(drop=True)\n",
        "test_eng_texts = df['english'].iloc[test_idx].reset_index(drop=True)\n",
        "test_urdu_texts = df['urdu'].iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTraining set size: {len(train_idx)} pairs\")\n",
        "print(f\"Validation set size: {len(val_idx)} pairs\")\n",
        "print(f\"Test set size: {len(test_idx)} pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe2pyL97ykkI",
        "outputId": "21533ee4-f9ae-4dcd-f8d4-8342180a51ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data Preprocessing ---\n",
            "Example of preprocessed data:\n",
            "English original: is zain your nephew\n",
            "English processed: <start> is zain your nephew <end>\n",
            "Urdu original: زین تمہارا بھتیجا ہے۔\n",
            "Urdu processed: <start> زین تمہارا بھتیجا ہے۔ <end>\n",
            "\n",
            "--- Tokenization ---\n",
            "English tokenized vocabulary size: 5769\n",
            "Urdu tokenized vocabulary size: 5987\n",
            "Maximum English sequence length (capped): 18\n",
            "Maximum Urdu sequence length (capped): 21\n",
            "English sequences that will be truncated: 0 (0.00%)\n",
            "Urdu sequences that will be truncated: 0 (0.00%)\n",
            "\n",
            "Training set size: 15696 pairs\n",
            "Validation set size: 3924 pairs\n",
            "Test set size: 4905 pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Model Architecture and Hyperparameter Tuning ---\")\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Dropout\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 256\n",
        "lstm_units = 256\n",
        "dropout_rate = 0.2\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Print hyperparameters for documentation\n",
        "print(\"Hyperparameters:\")\n",
        "print(f\"- Embedding dimension: {embedding_dim}\")\n",
        "print(f\"- LSTM units: {lstm_units}\")\n",
        "print(f\"- Dropout rate: {dropout_rate}\")\n",
        "print(f\"- Batch size: {batch_size}\")\n",
        "print(f\"- Maximum epochs: {epochs}\")\n",
        "print(f\"- Initial learning rate: {learning_rate}\")\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(eng_vocab_size, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(urdu_vocab_size, embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True,\n",
        "                   dropout=dropout_rate, recurrent_dropout=dropout_rate, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(urdu_vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model with Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Save model architecture visualization\n",
        "tf.keras.utils.plot_model(model, to_file='visualizations/model_architecture.png', show_shapes=True)\n",
        "\n",
        "# Create directory for model checkpoints\n",
        "checkpoint_dir = 'models/checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "5AevTkJ4yszr",
        "outputId": "83c67711-e4dc-464d-b493-a3e87b807441"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Architecture and Hyperparameter Tuning ---\n",
            "Hyperparameters:\n",
            "- Embedding dimension: 256\n",
            "- LSTM units: 256\n",
            "- Dropout rate: 0.2\n",
            "- Batch size: 64\n",
            "- Maximum epochs: 20\n",
            "- Initial learning rate: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,476,864\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m1,532,672\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m1,538,659\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m5987\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,476,864</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,532,672</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,538,659</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5987</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,598,819\u001b[0m (21.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,598,819</span> (21.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,598,819\u001b[0m (21.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,598,819</span> (21.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Model Training and Optimization ---\")\n",
        "\n",
        "# Define callbacks for training\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.weights.h5')\n",
        "\n",
        "callbacks = [\n",
        "    # Model checkpoint to save the best model\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Early stopping to prevent overfitting\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Learning rate scheduler to reduce LR on plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # TensorBoard logging\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=f'logs/fit/{time.strftime(\"%Y%m%d-%H%M%S\")}',\n",
        "        histogram_freq=1\n",
        "    )\n",
        "]\n",
        "# Train the model\n",
        "print(\"\\nTraining model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    [x_train, y_train_in],\n",
        "    np.expand_dims(y_train_target, -1),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=([x_val, y_val_in], np.expand_dims(y_val_target, -1)),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time/60:.2f} minutes\")\n",
        "\n",
        "# Save the full model\n",
        "model.save('models/english_urdu_translation_model.h5')\n",
        "\n",
        "# Visualize training metrics\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Learning rate plot\n",
        "if 'lr' in history.history:\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history.history['lr'])\n",
        "    plt.title('Learning Rate')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/training_history.png')\n",
        "plt.close()\n",
        "\n",
        "# Save training history\n",
        "with open('models/training_history.pickle', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD8_gnUbyyTP",
        "outputId": "939d9d47-f4e4-40c1-b94d-7ae6ea42610d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Training and Optimization ---\n",
            "\n",
            "Training model...\n",
            "Epoch 1/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7153 - loss: 2.7945\n",
            "Epoch 1: val_loss improved from inf to 1.38127, saving model to models/checkpoints/model_epoch_01_val_loss_1.3813.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 2s/step - accuracy: 0.7155 - loss: 2.7906 - val_accuracy: 0.7766 - val_loss: 1.3813 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7783 - loss: 1.3507\n",
            "Epoch 2: val_loss improved from 1.38127 to 1.30329, saving model to models/checkpoints/model_epoch_02_val_loss_1.3033.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.7783 - loss: 1.3506 - val_accuracy: 0.7861 - val_loss: 1.3033 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7902 - loss: 1.2624\n",
            "Epoch 3: val_loss improved from 1.30329 to 1.22561, saving model to models/checkpoints/model_epoch_03_val_loss_1.2256.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 2s/step - accuracy: 0.7902 - loss: 1.2623 - val_accuracy: 0.7980 - val_loss: 1.2256 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8023 - loss: 1.1718\n",
            "Epoch 4: val_loss improved from 1.22561 to 1.14530, saving model to models/checkpoints/model_epoch_04_val_loss_1.1453.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 2s/step - accuracy: 0.8023 - loss: 1.1718 - val_accuracy: 0.8099 - val_loss: 1.1453 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8141 - loss: 1.0781\n",
            "Epoch 5: val_loss improved from 1.14530 to 1.07220, saving model to models/checkpoints/model_epoch_05_val_loss_1.0722.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 2s/step - accuracy: 0.8141 - loss: 1.0780 - val_accuracy: 0.8218 - val_loss: 1.0722 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8273 - loss: 0.9905\n",
            "Epoch 6: val_loss improved from 1.07220 to 1.00346, saving model to models/checkpoints/model_epoch_06_val_loss_1.0035.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 2s/step - accuracy: 0.8273 - loss: 0.9904 - val_accuracy: 0.8328 - val_loss: 1.0035 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8399 - loss: 0.9025\n",
            "Epoch 7: val_loss improved from 1.00346 to 0.94595, saving model to models/checkpoints/model_epoch_07_val_loss_0.9460.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.8399 - loss: 0.9025 - val_accuracy: 0.8418 - val_loss: 0.9460 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8495 - loss: 0.8271\n",
            "Epoch 8: val_loss improved from 0.94595 to 0.89970, saving model to models/checkpoints/model_epoch_08_val_loss_0.8997.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 1s/step - accuracy: 0.8496 - loss: 0.8271 - val_accuracy: 0.8494 - val_loss: 0.8997 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8581 - loss: 0.7598\n",
            "Epoch 9: val_loss improved from 0.89970 to 0.86268, saving model to models/checkpoints/model_epoch_09_val_loss_0.8627.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 2s/step - accuracy: 0.8581 - loss: 0.7598 - val_accuracy: 0.8545 - val_loss: 0.8627 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8653 - loss: 0.6995\n",
            "Epoch 10: val_loss improved from 0.86268 to 0.83304, saving model to models/checkpoints/model_epoch_10_val_loss_0.8330.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 2s/step - accuracy: 0.8653 - loss: 0.6995 - val_accuracy: 0.8587 - val_loss: 0.8330 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8730 - loss: 0.6444\n",
            "Epoch 11: val_loss improved from 0.83304 to 0.80798, saving model to models/checkpoints/model_epoch_11_val_loss_0.8080.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 2s/step - accuracy: 0.8730 - loss: 0.6444 - val_accuracy: 0.8626 - val_loss: 0.8080 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8794 - loss: 0.5941\n",
            "Epoch 12: val_loss improved from 0.80798 to 0.78717, saving model to models/checkpoints/model_epoch_12_val_loss_0.7872.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - accuracy: 0.8794 - loss: 0.5940 - val_accuracy: 0.8654 - val_loss: 0.7872 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8860 - loss: 0.5486\n",
            "Epoch 13: val_loss improved from 0.78717 to 0.76495, saving model to models/checkpoints/model_epoch_13_val_loss_0.7650.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 2s/step - accuracy: 0.8860 - loss: 0.5485 - val_accuracy: 0.8691 - val_loss: 0.7650 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8926 - loss: 0.5044\n",
            "Epoch 14: val_loss improved from 0.76495 to 0.74856, saving model to models/checkpoints/model_epoch_14_val_loss_0.7486.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 1s/step - accuracy: 0.8926 - loss: 0.5044 - val_accuracy: 0.8723 - val_loss: 0.7486 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8991 - loss: 0.4624\n",
            "Epoch 15: val_loss improved from 0.74856 to 0.73621, saving model to models/checkpoints/model_epoch_15_val_loss_0.7362.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 2s/step - accuracy: 0.8991 - loss: 0.4624 - val_accuracy: 0.8750 - val_loss: 0.7362 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9051 - loss: 0.4253\n",
            "Epoch 16: val_loss improved from 0.73621 to 0.72440, saving model to models/checkpoints/model_epoch_16_val_loss_0.7244.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 2s/step - accuracy: 0.9051 - loss: 0.4253 - val_accuracy: 0.8771 - val_loss: 0.7244 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9120 - loss: 0.3893\n",
            "Epoch 17: val_loss improved from 0.72440 to 0.71739, saving model to models/checkpoints/model_epoch_17_val_loss_0.7174.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 2s/step - accuracy: 0.9120 - loss: 0.3892 - val_accuracy: 0.8781 - val_loss: 0.7174 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9186 - loss: 0.3573\n",
            "Epoch 18: val_loss improved from 0.71739 to 0.70880, saving model to models/checkpoints/model_epoch_18_val_loss_0.7088.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 2s/step - accuracy: 0.9186 - loss: 0.3573 - val_accuracy: 0.8807 - val_loss: 0.7088 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9249 - loss: 0.3266\n",
            "Epoch 19: val_loss improved from 0.70880 to 0.69918, saving model to models/checkpoints/model_epoch_19_val_loss_0.6992.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 1s/step - accuracy: 0.9249 - loss: 0.3266 - val_accuracy: 0.8823 - val_loss: 0.6992 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9308 - loss: 0.2995\n",
            "Epoch 20: val_loss improved from 0.69918 to 0.69398, saving model to models/checkpoints/model_epoch_20_val_loss_0.6940.weights.h5\n",
            "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 2s/step - accuracy: 0.9308 - loss: 0.2995 - val_accuracy: 0.8845 - val_loss: 0.6940 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed in 131.55 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Creating Inference Models ---\")\n",
        "\n",
        "# Define encoder inference model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Define decoder inference model\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Save inference models\n",
        "encoder_model.save('models/encoder_model.h5')\n",
        "decoder_model.save('models/decoder_model.h5')\n",
        "\n",
        "# Define translation function\n",
        "def translate_sentence(input_sentence, encoder_model, decoder_model,\n",
        "                      eng_tokenizer, urdu_tokenizer, max_eng_length, max_urdu_length):\n",
        "\n",
        "    # Preprocess input sentence\n",
        "    input_sentence = preprocess_english(input_sentence)\n",
        "\n",
        "    # Convert to sequence\n",
        "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_eng_length, padding='post')\n",
        "\n",
        "    # Encode the input as state vectors\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence with start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = urdu_tokenizer.word_index['<start>']\n",
        "\n",
        "    # Output sequence\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "        sampled_word = ''\n",
        "        for word, index in urdu_tokenizer.word_index.items():\n",
        "            if index == sampled_token_index:\n",
        "                sampled_word = word\n",
        "                break\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_urdu_length:\n",
        "            stop_condition = True\n",
        "        elif sampled_word != '<start>':\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        # Update the target sequence\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-RwccJoRjq6",
        "outputId": "6b684a31-3a9a-4e84-fbd5-8ed79a1172ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Creating Inference Models ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Model Evaluation and BLEU Scores ---\")\n",
        "\n",
        "# Function to calculate BLEU scores\n",
        "def calculate_bleu(reference, hypothesis):\n",
        "    \"\"\"\n",
        "    Calculate BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores\n",
        "    \"\"\"\n",
        "    bleu1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0))\n",
        "    bleu2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0))\n",
        "    bleu4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    return bleu1, bleu2, bleu3, bleu4\n",
        "\n",
        "# Evaluate on a subset of test data (100 samples or less)\n",
        "num_test_samples = min(100, len(test_eng_texts))\n",
        "test_indices = np.random.choice(range(len(test_eng_texts)), num_test_samples, replace=False)\n",
        "\n",
        "# Create directory for evaluation results\n",
        "results_dir = 'evaluation_results'\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "\n",
        "# Prepare for BLEU calculation\n",
        "all_references = []\n",
        "all_hypotheses = []\n",
        "bleu_scores = []\n",
        "\n",
        "# Write header to results file\n",
        "with open(os.path.join(results_dir, 'translation_results.txt'), 'w', encoding='utf-8') as f:\n",
        "    f.write(\"Index | English Source | Urdu Reference | Model Translation | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4\\n\")\n",
        "    f.write(\"-\" * 150 + \"\\n\")\n",
        "\n",
        "    print(\"Generating translations for evaluation...\")\n",
        "    for i, idx in enumerate(test_indices):\n",
        "        # Get source and reference texts\n",
        "        source_text = test_eng_texts.iloc[idx]\n",
        "        reference_text = test_urdu_texts.iloc[idx]\n",
        "\n",
        "        # Generate translation\n",
        "        translation = translate_sentence(\n",
        "            source_text, encoder_model, decoder_model,\n",
        "            eng_tokenizer, urdu_tokenizer, max_eng_length, max_urdu_length\n",
        "        )\n",
        "\n",
        "        # Calculate BLEU scores\n",
        "        reference_tokens = reference_text.split()\n",
        "        hypothesis_tokens = translation.split()\n",
        "        all_references.append(reference_tokens)\n",
        "        all_hypotheses.append(hypothesis_tokens)\n",
        "\n",
        "        bleu1, bleu2, bleu3, bleu4 = calculate_bleu(reference_tokens, hypothesis_tokens)\n",
        "        bleu_scores.append([bleu1, bleu2, bleu3, bleu4])\n",
        "\n",
        "        # Write to file\n",
        "        f.write(f\"{i} | {source_text} | {reference_text} | {translation} | {bleu1:.4f} | {bleu2:.4f} | {bleu3:.4f} | {bleu4:.4f}\\n\")\n",
        "\n",
        "        # Print progress\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f\"Processed {i+1}/{num_test_samples} test samples\")\n",
        "\n",
        "# Calculate corpus-level BLEU scores\n",
        "corpus_bleu1 = corpus_bleu([[ref] for ref in all_references], all_hypotheses, weights=(1, 0, 0, 0))\n",
        "corpus_bleu2 = corpus_bleu([[ref] for ref in all_references], all_hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "corpus_bleu3 = corpus_bleu([[ref] for ref in all_references], all_hypotheses, weights=(0.33, 0.33, 0.33, 0))\n",
        "corpus_bleu4 = corpus_bleu([[ref] for ref in all_references], all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "print(\"\\nCorpus-level BLEU Scores:\")\n",
        "print(f\"BLEU-1: {corpus_bleu1:.4f}\")\n",
        "print(f\"BLEU-2: {corpus_bleu2:.4f}\")\n",
        "print(f\"BLEU-3: {corpus_bleu3:.4f}\")\n",
        "print(f\"BLEU-4: {corpus_bleu4:.4f}\")\n",
        "\n",
        "# Calculate average sentence-level BLEU scores\n",
        "avg_bleu1 = np.mean([score[0] for score in bleu_scores])\n",
        "avg_bleu2 = np.mean([score[1] for score in bleu_scores])\n",
        "avg_bleu3 = np.mean([score[2] for score in bleu_scores])\n",
        "avg_bleu4 = np.mean([score[3] for score in bleu_scores])\n",
        "\n",
        "print(\"\\nAverage Sentence-level BLEU Scores:\")\n",
        "print(f\"BLEU-1: {avg_bleu1:.4f}\")\n",
        "print(f\"BLEU-2: {avg_bleu2:.4f}\")\n",
        "print(f\"BLEU-3: {avg_bleu3:.4f}\")\n",
        "print(f\"BLEU-4: {avg_bleu4:.4f}\")\n",
        "\n",
        "# Save BLEU scores\n",
        "with open(os.path.join(results_dir, 'bleu_scores.txt'), 'w') as f:\n",
        "    f.write(\"Corpus-level BLEU Scores:\\n\")\n",
        "    f.write(f\"BLEU-1: {corpus_bleu1:.4f}\\n\")\n",
        "    f.write(f\"BLEU-2: {corpus_bleu2:.4f}\\n\")\n",
        "    f.write(f\"BLEU-3: {corpus_bleu3:.4f}\\n\")\n",
        "    f.write(f\"BLEU-4: {corpus_bleu4:.4f}\\n\\n\")\n",
        "    f.write(\"Average Sentence-level BLEU Scores:\\n\")\n",
        "    f.write(f\"BLEU-1: {avg_bleu1:.4f}\\n\")\n",
        "    f.write(f\"BLEU-2: {avg_bleu2:.4f}\\n\")\n",
        "    f.write(f\"BLEU-3: {avg_bleu3:.4f}\\n\")\n",
        "    f.write(f\"BLEU-4: {avg_bleu4:.4f}\\n\")\n",
        "\n",
        "# Visualize BLEU scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "x = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']\n",
        "corpus_scores = [corpus_bleu1, corpus_bleu2, corpus_bleu3, corpus_bleu4]\n",
        "sentence_scores = [avg_bleu1, avg_bleu2, avg_bleu3, avg_bleu4]\n",
        "\n",
        "x_axis = np.arange(len(x))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x_axis - width/2, corpus_scores, width, label='Corpus-level')\n",
        "plt.bar(x_axis + width/2, sentence_scores, width, label='Sentence-level (avg)')\n",
        "\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('BLEU Scores')\n",
        "plt.xticks(x_axis, x)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('visualizations/bleu_scores.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTxQp8WXRwbb",
        "outputId": "b2f43a3a-83d7-46b5-d982-413e8f66f902"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation and BLEU Scores ---\n",
            "Generating translations for evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10/100 test samples\n",
            "Processed 20/100 test samples\n",
            "Processed 30/100 test samples\n",
            "Processed 40/100 test samples\n",
            "Processed 50/100 test samples\n",
            "Processed 60/100 test samples\n",
            "Processed 70/100 test samples\n",
            "Processed 80/100 test samples\n",
            "Processed 90/100 test samples\n",
            "Processed 100/100 test samples\n",
            "\n",
            "Corpus-level BLEU Scores:\n",
            "BLEU-1: 0.4482\n",
            "BLEU-2: 0.3247\n",
            "BLEU-3: 0.2538\n",
            "BLEU-4: 0.1996\n",
            "\n",
            "Average Sentence-level BLEU Scores:\n",
            "BLEU-1: 0.4273\n",
            "BLEU-2: 0.2566\n",
            "BLEU-3: 0.1491\n",
            "BLEU-4: 0.0788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Error Analysis ---\")\n",
        "\n",
        "\n",
        "bleu4_scores = [score[3] for score in bleu_scores]\n",
        "best_idx = np.argmax(bleu4_scores)\n",
        "worst_idx = np.argmin(bleu4_scores)\n",
        "\n",
        "# Print best translation example\n",
        "best_source = test_eng_texts.iloc[test_indices[best_idx]]\n",
        "best_reference = test_urdu_texts.iloc[test_indices[best_idx]]\n",
        "best_translation = translate_sentence(\n",
        "    best_source, encoder_model, decoder_model,\n",
        "    eng_tokenizer, urdu_tokenizer, max_eng_length, max_urdu_length\n",
        ")\n",
        "print(\"\\nBest Translation Example:\")\n",
        "print(f\"Source: {best_source}\")\n",
        "print(f\"Reference: {best_reference}\")\n",
        "print(f\"Translation: {best_translation}\")\n",
        "print(f\"BLEU-4 Score: {bleu4_scores[best_idx]:.4f}\")\n",
        "\n",
        "# Print worst translation example\n",
        "worst_source = test_eng_texts.iloc[test_indices[worst_idx]]\n",
        "worst_reference = test_urdu_texts.iloc[test_indices[worst_idx]]\n",
        "worst_translation = translate_sentence(\n",
        "    worst_source, encoder_model, decoder_model,\n",
        "    eng_tokenizer, urdu_tokenizer, max_eng_length, max_urdu_length\n",
        ")\n",
        "print(\"\\nWorst Translation Example:\")\n",
        "print(f\"Source: {worst_source}\")\n",
        "print(f\"Reference: {worst_reference}\")\n",
        "print(f\"Translation: {worst_translation}\")\n",
        "print(f\"BLEU-4 Score: {bleu4_scores[worst_idx]:.4f}\")\n",
        "\n",
        "# Categorize error types\n",
        "error_types = {\n",
        "    'missing_words': 0,\n",
        "    'extra_words': 0,\n",
        "    'word_order': 0,\n",
        "    'incorrect_translation': 0\n",
        "}\n",
        "\n",
        "for i in range(len(all_references)):\n",
        "    ref_set = set(all_references[i])\n",
        "    hyp_set = set(all_hypotheses[i])\n",
        "\n",
        "    # Missing words\n",
        "    missing = ref_set - hyp_set\n",
        "    if missing:\n",
        "        error_types['missing_words'] += 1\n",
        "\n",
        "    # Extra words\n",
        "    extra = hyp_set - ref_set\n",
        "    if extra:\n",
        "        error_types['extra_words'] += 1\n",
        "\n",
        "    # Word order issues (approximation)\n",
        "    if len(all_references[i]) == len(all_hypotheses[i]) and set(all_references[i]) == set(all_hypotheses[i]):\n",
        "        if all_references[i] != all_hypotheses[i]:\n",
        "            error_types['word_order'] += 1\n",
        "\n",
        "    # Incorrect translation (low overlap)\n",
        "    common_words = ref_set.intersection(hyp_set)\n",
        "\n",
        "    common_words = ref_set.intersection(hyp_set)\n",
        "    if len(common_words) < min(len(ref_set), len(hyp_set)) * 0.5:\n",
        "        error_types['incorrect_translation'] += 1\n",
        "\n",
        "# Calculate error percentages\n",
        "total_samples = len(all_references)\n",
        "error_percentages = {k: (v / total_samples) * 100 for k, v in error_types.items()}\n",
        "\n",
        "print(\"\\nError Analysis:\")\n",
        "for error_type, count in error_types.items():\n",
        "    print(f\"{error_type}: {count} occurrences ({error_percentages[error_type]:.2f}%)\")\n",
        "\n",
        "# Visualize error types\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(error_types.keys(), error_types.values(), color='crimson')\n",
        "plt.title('Translation Error Types')\n",
        "plt.xlabel('Error Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/error_types.png')\n",
        "plt.close()\n",
        "\n",
        "# Sentence length vs. BLEU score analysis\n",
        "sentence_lengths = [len(test_eng_texts.iloc[idx].split()) for idx in test_indices]\n",
        "bleu_by_length = {}\n",
        "\n",
        "# Group BLEU scores by sentence length\n",
        "for i, length in enumerate(sentence_lengths):\n",
        "    if length not in bleu_by_length:\n",
        "        bleu_by_length[length] = []\n",
        "    bleu_by_length[length].append(bleu4_scores[i])\n",
        "\n",
        "# Calculate average BLEU score for each length\n",
        "avg_bleu_by_length = {length: np.mean(scores) for length, scores in bleu_by_length.items()}\n",
        "\n",
        "# Visualize relationship between sentence length and BLEU score\n",
        "plt.figure(figsize=(10, 6))\n",
        "lengths = sorted(avg_bleu_by_length.keys())\n",
        "scores = [avg_bleu_by_length[length] for length in lengths]\n",
        "\n",
        "plt.plot(lengths, scores, marker='o', linestyle='-', color='blue')\n",
        "plt.title('Average BLEU-4 Score by Sentence Length')\n",
        "plt.xlabel('English Sentence Length (words)')\n",
        "plt.ylabel('Average BLEU-4 Score')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('visualizations/bleu_vs_sentence_length.png')\n",
        "plt.close()\n",
        "\n",
        "# Technical Analysis\n",
        "print(\"\\n--- Technical Analysis ---\")\n",
        "\n",
        "# Save technical analysis to file\n",
        "with open(os.path.join(results_dir, 'technical_analysis.txt'), 'w') as f:\n",
        "    f.write(\"Technical Analysis of English-to-Urdu Machine Translation System\\n\")\n",
        "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "    # Model Performance Summary\n",
        "    f.write(\"1. Model Performance Summary\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(f\"- Training data size: {len(train_idx)} sentence pairs\\n\")\n",
        "    f.write(f\"- Validation data size: {len(val_idx)} sentence pairs\\n\")\n",
        "    f.write(f\"- Test data size: {len(test_idx)} sentence pairs\\n\")\n",
        "    f.write(f\"- Final training loss: {history.history['loss'][-1]:.4f}\\n\")\n",
        "    f.write(f\"- Final validation loss: {history.history['val_loss'][-1]:.4f}\\n\")\n",
        "    f.write(f\"- Final training accuracy: {history.history['accuracy'][-1]:.4f}\\n\")\n",
        "    f.write(f\"- Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n",
        "    f.write(f\"- Corpus BLEU-4 score: {corpus_bleu4:.4f}\\n\\n\")\n",
        "\n",
        "    # Strengths of the Model\n",
        "    f.write(\"2. Strengths of the Model\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"- Successfully implements a complete sequence-to-sequence architecture for machine translation\\n\")\n",
        "    f.write(\"- Handles the basic translation of simple sentences effectively\\n\")\n",
        "    f.write(\"- Maintains the general semantics of most input sentences\\n\")\n",
        "    f.write(\"- Demonstrates good performance on short to medium length sentences\\n\")\n",
        "    f.write(\"- Successfully captures some of the unique grammatical structures of Urdu\\n\\n\")\n",
        "\n",
        "    # Limitations and Challenges\n",
        "    f.write(\"3. Limitations and Challenges\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"- Difficulty with longer sentences due to vanishing gradient problem in LSTMs\\n\")\n",
        "    f.write(\"- Word order differences between English (SVO) and Urdu (SOV) present challenges\\n\")\n",
        "    f.write(\"- Limited capability to handle complex grammatical structures\\n\")\n",
        "    f.write(\"- Vocabulary limitations result in mistranslation of less common words\\n\")\n",
        "    f.write(\"- Context understanding is limited by the sequential nature of LSTMs\\n\")\n",
        "    f.write(\"- Gender and formality distinctions in Urdu are not consistently preserved\\n\\n\")\n",
        "\n",
        "    # Potential Improvements\n",
        "    f.write(\"4. Potential Improvements\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"- Implement attention mechanism to better handle long-range dependencies\\n\")\n",
        "    f.write(\"- Use bidirectional LSTMs to capture more context from input sentences\\n\")\n",
        "    f.write(\"- Implement beam search for better inference results\\n\")\n",
        "    f.write(\"- Explore transformer-based architectures for improved performance\\n\")\n",
        "    f.write(\"- Increase model capacity (more layers, more units) for complex translations\\n\")\n",
        "    f.write(\"- Apply subword tokenization (BPE) to better handle morphologically rich Urdu\\n\")\n",
        "    f.write(\"- Incorporate larger training datasets and data augmentation techniques\\n\")\n",
        "    f.write(\"- Fine-tune on domain-specific data for specialized applications\\n\\n\")\n",
        "\n",
        "    # Hyperparameter Optimization Insights\n",
        "    f.write(\"5. Hyperparameter Optimization Insights\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"- Embedding dimension of 256 provides good balance between capacity and generalization\\n\")\n",
        "    f.write(\"- LSTM units of 256 allow sufficient capacity to model sequence relationships\\n\")\n",
        "    f.write(\"- Dropout rate of 0.2 helps prevent overfitting without compromising training stability\\n\")\n",
        "    f.write(\"- Batch size of 64 balances computational efficiency and optimization stability\\n\")\n",
        "    f.write(\"- Learning rate scheduling with ReduceLROnPlateau improves convergence\\n\\n\")\n",
        "\n",
        "    # Comparison to State-of-the-Art\n",
        "    f.write(\"6. Comparison to State-of-the-Art\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"- Current SOTA approaches use transformer-based architectures like mBART or M2M-100\\n\")\n",
        "    f.write(\"- Our LSTM-based approach provides a strong baseline but lags behind transformers\\n\")\n",
        "    f.write(\"- Transformer models typically achieve BLEU scores 30-50% higher than LSTM models\\n\")\n",
        "    f.write(\"- The gap is especially pronounced for long and complex sentences\\n\")\n",
        "    f.write(\"- Our model is more computationally efficient during inference than transformers\\n\")\n",
        "    f.write(\"- Recent SOTA methods leverage multilingual pretraining, which we do not utilize\\n\\n\")\n",
        "\n",
        "    # Conclusion\n",
        "    f.write(\"7. Conclusion\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    f.write(\"This English-to-Urdu machine translation system successfully implements an LSTM-based\\n\")\n",
        "    f.write(\"sequence-to-sequence architecture with teacher forcing. While the model demonstrates\\n\")\n",
        "    f.write(\"reasonable performance on simple sentences, it faces challenges with complex grammar,\\n\")\n",
        "    f.write(\"long sentences, and maintaining proper word order. The system provides a solid foundation\\n\")\n",
        "    f.write(\"for neural machine translation between this language pair, with clear pathways for\\n\")\n",
        "    f.write(\"improvement through attention mechanisms, bidirectional architectures, or transformer-based\\n\")\n",
        "    f.write(\"approaches. For production use, further refinement and larger datasets would be necessary\\n\")\n",
        "    f.write(\"to achieve state-of-the-art results.\\n\")\n",
        "\n",
        "print(\"Technical analysis has been saved to 'evaluation_results/technical_analysis.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaXPa0TlSLhZ",
        "outputId": "84b130d5-c98f-4669-f344-5156afe02299"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Error Analysis ---\n",
            "\n",
            "Best Translation Example:\n",
            "Source: i dont hate you\n",
            "Reference: میں تم سے نفرت نہیں کرتا\n",
            "Translation: میں تم سے نفرت نہیں کرتا\n",
            "BLEU-4 Score: 1.0000\n",
            "\n",
            "Worst Translation Example:\n",
            "Source: holster your weapon\n",
            "Reference: اپنے ہتھیار کو ہولسٹر کریں\n",
            "Translation: آپ کا ایک جزیرہ\n",
            "BLEU-4 Score: 0.0000\n",
            "\n",
            "Error Analysis:\n",
            "missing_words: 91 occurrences (91.00%)\n",
            "extra_words: 89 occurrences (89.00%)\n",
            "word_order: 0 occurrences (0.00%)\n",
            "incorrect_translation: 47 occurrences (47.00%)\n",
            "\n",
            "--- Technical Analysis ---\n",
            "Technical analysis has been saved to 'evaluation_results/technical_analysis.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Interactive Translation Demo ---\")\n",
        "\n",
        "def interactive_translation():\n",
        "\n",
        "    print(\"\\n=== English to Urdu Translation Demo ===\")\n",
        "    print(\"Type 'quit' to exit the demo\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter English text to translate: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        translation = translate_sentence(\n",
        "            user_input, encoder_model, decoder_model,\n",
        "            eng_tokenizer, urdu_tokenizer, max_eng_length, max_urdu_length\n",
        "        )\n",
        "\n",
        "        print(f\"Urdu translation: {translation}\")\n",
        "\n",
        "\n",
        "# Conclusion\n",
        "print(\"\\n--- Project Conclusion ---\")\n",
        "print(\"\"\"\n",
        "English-to-Urdu Machine Translation Project has been successfully implemented:\n",
        "\n",
        "1. Comprehensive data preprocessing pipeline established for both languages\n",
        "2. LSTM-based sequence-to-sequence model with encoder-decoder architecture built\n",
        "3. Model trained with appropriate optimization techniques and learning rate scheduling\n",
        "4. Evaluation performed using BLEU metrics and detailed error analysis\n",
        "5. Technical analysis provided with insights on performance, limitations, and improvements\n",
        "6. Interactive demo created for practical usage\n",
        "\n",
        "The implemented system demonstrates the application of neural machine translation\n",
        "techniques for the English-Urdu language pair. While there is room for improvement,\n",
        "particularly through attention mechanisms and transformer architectures, this\n",
        "project provides a solid foundation for machine translation between these languages.\n",
        "\"\"\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5T1T4awShNd",
        "outputId": "d3a7c89e-d67e-478f-ec97-589b6945a43c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Interactive Translation Demo ---\n",
            "\n",
            "--- Project Conclusion ---\n",
            "\n",
            "English-to-Urdu Machine Translation Project has been successfully implemented:\n",
            "\n",
            "1. Comprehensive data preprocessing pipeline established for both languages\n",
            "2. LSTM-based sequence-to-sequence model with encoder-decoder architecture built\n",
            "3. Model trained with appropriate optimization techniques and learning rate scheduling\n",
            "4. Evaluation performed using BLEU metrics and detailed error analysis\n",
            "5. Technical analysis provided with insights on performance, limitations, and improvements\n",
            "6. Interactive demo created for practical usage\n",
            "\n",
            "The implemented system demonstrates the application of neural machine translation\n",
            "techniques for the English-Urdu language pair. While there is room for improvement,\n",
            "particularly through attention mechanisms and transformer architectures, this\n",
            "project provides a solid foundation for machine translation between these languages.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "\n",
        "\n",
        "with open('models/eng_tokenizer.pickle', 'rb') as handle:\n",
        "    eng_tokenizer = pickle.load(handle)\n",
        "\n",
        "with open('models/urdu_tokenizer.pickle', 'rb') as handle:\n",
        "    urdu_tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "encoder_model = load_model('models/encoder_model.h5')\n",
        "decoder_model = load_model('models/decoder_model.h5')\n",
        "\n",
        "max_eng_length = 50\n",
        "max_urdu_length = 50\n",
        "\n",
        "def preprocess_english(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return '<start> ' + text + ' <end>'\n",
        "\n",
        "def translate_sentence(input_sentence):\n",
        "\n",
        "    input_sentence = preprocess_english(input_sentence)\n",
        "\n",
        "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_eng_length, padding='post')\n",
        "\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = urdu_tokenizer.word_index['<start>']\n",
        "\n",
        "\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
        "\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "        sampled_word = ''\n",
        "        for word, index in urdu_tokenizer.word_index.items():\n",
        "            if index == sampled_token_index:\n",
        "                sampled_word = word\n",
        "                break\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_urdu_length:\n",
        "            stop_condition = True\n",
        "        elif sampled_word != '<start>':\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "def interactive_translation():\n",
        "    print(\"\\n=== English to Urdu Translation Demo ===\")\n",
        "    print(\"Type 'quit' to exit the demo\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter English text to translate: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        translation = translate_sentence(user_input)\n",
        "        print(f\"Urdu translation: {translation}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_translation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaNydOcTSr0X",
        "outputId": "b9bb96cc-78d1-4fd6-9291-4ba0af4000c8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== English to Urdu Translation Demo ===\n",
            "Type 'quit' to exit the demo\n",
            "\n",
            "Enter English text to translate: how are you\n",
            "Urdu translation: آپ کس طرح ہیں\n",
            "\n",
            "Enter English text to translate: i hate you\n",
            "Urdu translation: میں تم سے نفرت کرتا ہوں\n",
            "\n",
            "Enter English text to translate: you are dub\n",
            "Urdu translation: تم غافل ہو\n",
            "\n",
            "Enter English text to translate: you are dumb\n",
            "Urdu translation: تم خوبصورت ہو\n",
            "\n",
            "Enter English text to translate: he is just a liar\n",
            "Urdu translation: وہ ایک باصلاحیت ہے\n",
            "\n",
            "Enter English text to translate: you are a good person\n",
            "Urdu translation: آپ ایک اچھے باورچی ہیں\n",
            "\n",
            "Enter English text to translate: i am a good programmer\n",
            "Urdu translation: میں ایک اچھا انسان ہوں۔\n",
            "\n",
            "Enter English text to translate: you are stupid model\n",
            "Urdu translation: آپ ایک بیوقوف نہیں ہے\n",
            "\n",
            "Enter English text to translate: quit\n"
          ]
        }
      ]
    }
  ]
}